{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN                                      NAME APPLICATION_TYPE  \\\n",
       "0  10520599              BLUE KNIGHTS MOTORCYCLE CLUB              T10   \n",
       "1  10531628    AMERICAN CHESAPEAKE CLUB CHARITABLE TR               T3   \n",
       "2  10547893        ST CLOUD PROFESSIONAL FIREFIGHTERS               T5   \n",
       "3  10553066            SOUTHSIDE ATHLETIC ASSOCIATION               T3   \n",
       "4  10556103  GENETIC RESEARCH INSTITUTE OF THE DESERT               T3   \n",
       "\n",
       "        AFFILIATION CLASSIFICATION      USE_CASE  ORGANIZATION  STATUS  \\\n",
       "0       Independent          C1000    ProductDev   Association       1   \n",
       "1       Independent          C2000  Preservation  Co-operative       1   \n",
       "2  CompanySponsored          C3000    ProductDev   Association       1   \n",
       "3  CompanySponsored          C2000  Preservation         Trust       1   \n",
       "4       Independent          C1000     Heathcare         Trust       1   \n",
       "\n",
       "      INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0              0                      N     5000              1  \n",
       "1         1-9999                      N   108590              1  \n",
       "2              0                      N     5000              0  \n",
       "3    10000-24999                      N     6692              1  \n",
       "4  100000-499999                      N   142590              1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n",
    "application_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34294</th>\n",
       "      <td>T4</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34295</th>\n",
       "      <td>T4</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34297</th>\n",
       "      <td>T5</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1M-5M</td>\n",
       "      <td>N</td>\n",
       "      <td>36500179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34299 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0                  T10       Independent          C1000    ProductDev   \n",
       "1                   T3       Independent          C2000  Preservation   \n",
       "2                   T5  CompanySponsored          C3000    ProductDev   \n",
       "3                   T3  CompanySponsored          C2000  Preservation   \n",
       "4                   T3       Independent          C1000     Heathcare   \n",
       "...                ...               ...            ...           ...   \n",
       "34294               T4       Independent          C1000    ProductDev   \n",
       "34295               T4  CompanySponsored          C3000    ProductDev   \n",
       "34296               T3  CompanySponsored          C2000  Preservation   \n",
       "34297               T5       Independent          C3000    ProductDev   \n",
       "34298               T3       Independent          C1000  Preservation   \n",
       "\n",
       "       ORGANIZATION  STATUS     INCOME_AMT SPECIAL_CONSIDERATIONS   ASK_AMT  \\\n",
       "0       Association       1              0                      N      5000   \n",
       "1      Co-operative       1         1-9999                      N    108590   \n",
       "2       Association       1              0                      N      5000   \n",
       "3             Trust       1    10000-24999                      N      6692   \n",
       "4             Trust       1  100000-499999                      N    142590   \n",
       "...             ...     ...            ...                    ...       ...   \n",
       "34294   Association       1              0                      N      5000   \n",
       "34295   Association       1              0                      N      5000   \n",
       "34296   Association       1              0                      N      5000   \n",
       "34297   Association       1              0                      N      5000   \n",
       "34298  Co-operative       1          1M-5M                      N  36500179   \n",
       "\n",
       "       IS_SUCCESSFUL  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  0  \n",
       "3                  1  \n",
       "4                  1  \n",
       "...              ...  \n",
       "34294              0  \n",
       "34295              0  \n",
       "34296              0  \n",
       "34297              1  \n",
       "34298              0  \n",
       "\n",
       "[34299 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df = application_df.drop(columns=['EIN','NAME'])\n",
    "application_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EIN                       34299\n",
       "NAME                      19568\n",
       "APPLICATION_TYPE             17\n",
       "AFFILIATION                   6\n",
       "CLASSIFICATION               71\n",
       "USE_CASE                      5\n",
       "ORGANIZATION                  4\n",
       "STATUS                        2\n",
       "INCOME_AMT                    9\n",
       "SPECIAL_CONSIDERATIONS        2\n",
       "ASK_AMT                    8747\n",
       "IS_SUCCESSFUL                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "application_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3     27037\n",
       "T4      1542\n",
       "T6      1216\n",
       "T5      1173\n",
       "T19     1065\n",
       "T8       737\n",
       "T7       725\n",
       "T10      528\n",
       "T9       156\n",
       "T13       66\n",
       "T12       27\n",
       "T2        16\n",
       "T14        3\n",
       "T25        3\n",
       "T15        2\n",
       "T29        2\n",
       "T17        1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "count_type = application_df['APPLICATION_TYPE'].value_counts()\n",
    "count_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3       27037\n",
       "T4        1542\n",
       "T6        1216\n",
       "T5        1173\n",
       "T19       1065\n",
       "T8         737\n",
       "T7         725\n",
       "T10        528\n",
       "Other      276\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "application_types_to_replace =list(count_type.iloc[8:].index)\n",
    "cutoff_value = 528\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "application_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "         ...  \n",
       "C5200        1\n",
       "C4500        1\n",
       "C2600        1\n",
       "C1570        1\n",
       "C1728        1\n",
       "Name: CLASSIFICATION, Length: 71, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "classification= application_df['CLASSIFICATION'].value_counts()\n",
    "classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other    32038\n",
       "C7000      777\n",
       "C1700      287\n",
       "C4000      194\n",
       "C5000      116\n",
       "         ...  \n",
       "C5200        1\n",
       "C2600        1\n",
       "C4500        1\n",
       "C1570        1\n",
       "C1728        1\n",
       "Name: CLASSIFICATION, Length: 67, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "classifications_to_replace= list(classification.iloc[:5].index)\n",
    "cutoff_value_classification=1883\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "      <th>NAME_1 DAY RANCH RESCUE AND RURAL OKLAHOMA ANIMAL RESOURCE INC</th>\n",
       "      <th>NAME_100 BLACK MEN OF AMERICA</th>\n",
       "      <th>NAME_100 BLACK MEN OF MEMPHIS INC</th>\n",
       "      <th>NAME_100 BLACK MEN OF WEST GEORGIA INC</th>\n",
       "      <th>NAME_1150 WEBSTER STREET INC</th>\n",
       "      <th>NAME_116TH CAVALRY REGIMENT CHAPTER OF THE US CAVALRY &amp; ARMOR ASSOCIATION</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN  STATUS  ASK_AMT  IS_SUCCESSFUL  \\\n",
       "0  10520599       1     5000              1   \n",
       "1  10531628       1   108590              1   \n",
       "2  10547893       1     5000              0   \n",
       "3  10553066       1     6692              1   \n",
       "4  10556103       1   142590              1   \n",
       "\n",
       "   NAME_1 DAY RANCH RESCUE AND RURAL OKLAHOMA ANIMAL RESOURCE INC  \\\n",
       "0                                                  0                \n",
       "1                                                  0                \n",
       "2                                                  0                \n",
       "3                                                  0                \n",
       "4                                                  0                \n",
       "\n",
       "   NAME_100 BLACK MEN OF AMERICA  NAME_100 BLACK MEN OF MEMPHIS INC  \\\n",
       "0                              0                                  0   \n",
       "1                              0                                  0   \n",
       "2                              0                                  0   \n",
       "3                              0                                  0   \n",
       "4                              0                                  0   \n",
       "\n",
       "   NAME_100 BLACK MEN OF WEST GEORGIA INC  NAME_1150 WEBSTER STREET INC  \\\n",
       "0                                       0                             0   \n",
       "1                                       0                             0   \n",
       "2                                       0                             0   \n",
       "3                                       0                             0   \n",
       "4                                       0                             0   \n",
       "\n",
       "   NAME_116TH CAVALRY REGIMENT CHAPTER OF THE US CAVALRY & ARMOR ASSOCIATION  \\\n",
       "0                                                  0                           \n",
       "1                                                  0                           \n",
       "2                                                  0                           \n",
       "3                                                  0                           \n",
       "4                                                  0                           \n",
       "\n",
       "   ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n",
       "0  ...                  0                       0                         0   \n",
       "1  ...                  1                       0                         0   \n",
       "2  ...                  0                       0                         0   \n",
       "3  ...                  0                       1                         0   \n",
       "4  ...                  0                       0                         1   \n",
       "\n",
       "   INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n",
       "0                   0                 0                       0   \n",
       "1                   0                 0                       0   \n",
       "2                   0                 0                       0   \n",
       "3                   0                 0                       0   \n",
       "4                   0                 0                       0   \n",
       "\n",
       "   INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n",
       "0                0                  0                         1   \n",
       "1                0                  0                         1   \n",
       "2                0                  0                         1   \n",
       "3                0                  0                         1   \n",
       "4                0                  0                         1   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_Y  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "\n",
       "[5 rows x 19682 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "number_numeric= pd.get_dummies(application_df)\n",
    "number_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.77 GiB for an array with shape (25724, 19681) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32124\\792047424.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Split the preprocessed data into a training and testing dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2443\u001b[0m     return list(\n\u001b[0;32m   2444\u001b[0m         chain.from_iterable(\n\u001b[1;32m-> 2445\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2446\u001b[0m         )\n\u001b[0;32m   2447\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2443\u001b[0m     return list(\n\u001b[0;32m   2444\u001b[0m         chain.from_iterable(\n\u001b[1;32m-> 2445\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2446\u001b[0m         )\n\u001b[0;32m   2447\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.77 GiB for an array with shape (25724, 19681) and data type int64"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "y = number_numeric['IS_SUCCESSFUL'].values\n",
    "X = number_numeric.drop(columns='IS_SUCCESSFUL').values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 840       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 891\n",
      "Trainable params: 891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "input_number = len(X_train[0])\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim = input_number, activation = \"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\19802\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.6086 - acc: 0.6935\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5724 - acc: 0.7227\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 2s 81us/sample - loss: 0.5670 - acc: 0.7242\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5642 - acc: 0.7248\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5627 - acc: 0.7268\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5613 - acc: 0.72670s - loss: 0.5586 - \n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5602 - acc: 0.7264\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5599 - acc: 0.7267\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5597 - acc: 0.7275\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5590 - acc: 0.7281\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5589 - acc: 0.7278\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5590 - acc: 0.72820s - loss: 0.5587 - acc: 0.\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5585 - acc: 0.7287\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5582 - acc: 0.7289\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.5582 - acc: 0.7294\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5579 - acc: 0.7284\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5579 - acc: 0.7282\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.5574 - acc: 0.7275\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.5576 - acc: 0.72801s - loss: 0.\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5573 - acc: 0.7290\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5570 - acc: 0.7281\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.5570 - acc: 0.7284\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5568 - acc: 0.7284\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5568 - acc: 0.7296\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5567 - acc: 0.7284\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5566 - acc: 0.7291\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 3s 100us/sample - loss: 0.5564 - acc: 0.7289\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5562 - acc: 0.72931s - lo\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5564 - acc: 0.7300\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5563 - acc: 0.7291\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5560 - acc: 0.7286\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5561 - acc: 0.7283\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5559 - acc: 0.7287\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5562 - acc: 0.7286\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5557 - acc: 0.7297\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5558 - acc: 0.7292\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5559 - acc: 0.7294\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5559 - acc: 0.7299\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5556 - acc: 0.7299\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5556 - acc: 0.7284\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5556 - acc: 0.7296\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5557 - acc: 0.7296\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5555 - acc: 0.7293\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5555 - acc: 0.7297\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5556 - acc: 0.73000s - loss: 0.5590 \n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5554 - acc: 0.7295\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5553 - acc: 0.7295\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5554 - acc: 0.7300\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5552 - acc: 0.7298\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5554 - acc: 0.7288\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5553 - acc: 0.7294\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5552 - acc: 0.7300\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5554 - acc: 0.72971s - loss: 0.5618 - acc: 0. - ETA: 0s - loss: \n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5553 - acc: 0.7307\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5550 - acc: 0.7302\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5552 - acc: 0.7300\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5549 - acc: 0.7295\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5551 - acc: 0.7294\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5548 - acc: 0.7294\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5548 - acc: 0.7297\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5547 - acc: 0.7302\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5550 - acc: 0.7301\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5549 - acc: 0.7301\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5548 - acc: 0.7294\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5547 - acc: 0.73081s - loss: \n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5548 - acc: 0.7299\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 3s 106us/sample - loss: 0.5546 - acc: 0.7298\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.5548 - acc: 0.7304\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5547 - acc: 0.7303\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5547 - acc: 0.7297\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5547 - acc: 0.7303\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5547 - acc: 0.7306\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5545 - acc: 0.73031s\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5548 - acc: 0.73020s - loss: 0.5554 - acc: 0\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5544 - acc: 0.7305\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5544 - acc: 0.7313\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5545 - acc: 0.73041s - lo\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5543 - acc: 0.73030s - loss: 0.5550 - acc: 0\n",
      "Epoch 79/100\n",
      "25724/25724 [==============================] - 2s 78us/sample - loss: 0.5544 - acc: 0.7303\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5546 - acc: 0.7289\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5543 - acc: 0.7296\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5544 - acc: 0.7309\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5544 - acc: 0.7300\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5544 - acc: 0.7301\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5544 - acc: 0.7304\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5543 - acc: 0.7299\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5543 - acc: 0.73040s - loss: 0.5525 -\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5543 - acc: 0.7301\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5544 - acc: 0.7305\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 2s 69us/sample - loss: 0.5543 - acc: 0.73060s - loss: 0.5525 -\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5542 - acc: 0.7308\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5543 - acc: 0.7307\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5543 - acc: 0.7307\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 2s 67us/sample - loss: 0.5542 - acc: 0.7297\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5543 - acc: 0.7311\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 2s 76us/sample - loss: 0.5545 - acc: 0.7297\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 2s 73us/sample - loss: 0.5542 - acc: 0.7306\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5543 - acc: 0.7311\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5542 - acc: 0.7294\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5541 - acc: 0.7302\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8575/8575 - 0s - loss: 0.5631 - acc: 0.7333\n",
      "Loss: 0.5631241679747667, Accuracy: 0.7332944869995117\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 7)                 735       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 14)                112       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 21)                315       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 22        \n",
      "=================================================================\n",
      "Total params: 1,184\n",
      "Trainable params: 1,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "input_dim = len(X_train[0])\n",
    "hidden_nodes_layer1 = 7\n",
    "hidden_nodes_layer2 = 14\n",
    "hidden_nodes_layer3 = 21\n",
    "\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim = input_dim, activation = \"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.6068 - acc: 0.6891\n",
      "Epoch 2/200\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5722 - acc: 0.7240\n",
      "Epoch 3/200\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5671 - acc: 0.7263\n",
      "Epoch 4/200\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5644 - acc: 0.7263\n",
      "Epoch 5/200\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.5624 - acc: 0.7276\n",
      "Epoch 6/200\n",
      "25724/25724 [==============================] - 2s 72us/sample - loss: 0.5617 - acc: 0.7273\n",
      "Epoch 7/200\n",
      "25724/25724 [==============================] - 2s 78us/sample - loss: 0.5609 - acc: 0.7283\n",
      "Epoch 8/200\n",
      "25724/25724 [==============================] - 2s 88us/sample - loss: 0.5604 - acc: 0.7282\n",
      "Epoch 9/200\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5598 - acc: 0.7279\n",
      "Epoch 10/200\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5593 - acc: 0.7277\n",
      "Epoch 11/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5589 - acc: 0.72830s - loss: 0.\n",
      "Epoch 12/200\n",
      "25724/25724 [==============================] - 2s 77us/sample - loss: 0.5583 - acc: 0.7290\n",
      "Epoch 13/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5578 - acc: 0.7295\n",
      "Epoch 14/200\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5583 - acc: 0.72950s - loss: 0\n",
      "Epoch 15/200\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5577 - acc: 0.7292\n",
      "Epoch 16/200\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5573 - acc: 0.7289\n",
      "Epoch 17/200\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5570 - acc: 0.7289\n",
      "Epoch 18/200\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5570 - acc: 0.7290\n",
      "Epoch 19/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5570 - acc: 0.7303\n",
      "Epoch 20/200\n",
      "25724/25724 [==============================] - 2s 75us/sample - loss: 0.5567 - acc: 0.7285\n",
      "Epoch 21/200\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5564 - acc: 0.7305\n",
      "Epoch 22/200\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5565 - acc: 0.7289\n",
      "Epoch 23/200\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5563 - acc: 0.7302\n",
      "Epoch 24/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5559 - acc: 0.7299\n",
      "Epoch 25/200\n",
      "25724/25724 [==============================] - 2s 65us/sample - loss: 0.5560 - acc: 0.7300\n",
      "Epoch 26/200\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5560 - acc: 0.73030s - loss: 0.5571 - acc: 0.72\n",
      "Epoch 27/200\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5558 - acc: 0.7301\n",
      "Epoch 28/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5554 - acc: 0.7300\n",
      "Epoch 29/200\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5559 - acc: 0.7304\n",
      "Epoch 30/200\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5555 - acc: 0.7296\n",
      "Epoch 31/200\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5552 - acc: 0.7306\n",
      "Epoch 32/200\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5554 - acc: 0.7313\n",
      "Epoch 33/200\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5551 - acc: 0.73013s - loss: 0.5517 - acc: 0.731 - ETA: 3s - l\n",
      "Epoch 34/200\n",
      "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5556 - acc: 0.7305\n",
      "Epoch 35/200\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5552 - acc: 0.7307\n",
      "Epoch 36/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5549 - acc: 0.7294\n",
      "Epoch 37/200\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5551 - acc: 0.73060s - loss: 0.5551 - acc: 0.730\n",
      "Epoch 38/200\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5550 - acc: 0.7304\n",
      "Epoch 39/200\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5548 - acc: 0.7308\n",
      "Epoch 40/200\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5547 - acc: 0.7322\n",
      "Epoch 41/200\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.5551 - acc: 0.7304\n",
      "Epoch 42/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5548 - acc: 0.7301\n",
      "Epoch 43/200\n",
      "25724/25724 [==============================] - 3s 98us/sample - loss: 0.5545 - acc: 0.73091s\n",
      "Epoch 44/200\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5544 - acc: 0.7317\n",
      "Epoch 45/200\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5544 - acc: 0.7314\n",
      "Epoch 46/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5547 - acc: 0.7311\n",
      "Epoch 47/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5542 - acc: 0.7306\n",
      "Epoch 48/200\n",
      "25724/25724 [==============================] - 1s 37us/sample - loss: 0.5544 - acc: 0.7316\n",
      "Epoch 49/200\n",
      "25724/25724 [==============================] - 1s 37us/sample - loss: 0.5543 - acc: 0.7308\n",
      "Epoch 50/200\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5544 - acc: 0.7301\n",
      "Epoch 51/200\n",
      "25724/25724 [==============================] - 2s 62us/sample - loss: 0.5542 - acc: 0.7317\n",
      "Epoch 52/200\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5541 - acc: 0.7307\n",
      "Epoch 53/200\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5540 - acc: 0.7303\n",
      "Epoch 54/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5542 - acc: 0.7313\n",
      "Epoch 55/200\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5539 - acc: 0.7305\n",
      "Epoch 56/200\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5540 - acc: 0.7317\n",
      "Epoch 57/200\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5539 - acc: 0.7312\n",
      "Epoch 58/200\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5536 - acc: 0.7319\n",
      "Epoch 59/200\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5535 - acc: 0.7306\n",
      "Epoch 60/200\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5539 - acc: 0.7315\n",
      "Epoch 61/200\n",
      "25724/25724 [==============================] - 1s 37us/sample - loss: 0.5532 - acc: 0.7315\n",
      "Epoch 62/200\n",
      "25724/25724 [==============================] - 1s 38us/sample - loss: 0.5537 - acc: 0.7311\n",
      "Epoch 63/200\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5536 - acc: 0.7310\n",
      "Epoch 64/200\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5536 - acc: 0.7315\n",
      "Epoch 65/200\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5534 - acc: 0.7301\n",
      "Epoch 66/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5534 - acc: 0.7308\n",
      "Epoch 67/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5535 - acc: 0.7313\n",
      "Epoch 68/200\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5533 - acc: 0.7321\n",
      "Epoch 69/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5531 - acc: 0.7323\n",
      "Epoch 70/200\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5532 - acc: 0.7324\n",
      "Epoch 71/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5532 - acc: 0.7311\n",
      "Epoch 72/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5533 - acc: 0.7312\n",
      "Epoch 73/200\n",
      "25724/25724 [==============================] - 1s 38us/sample - loss: 0.5534 - acc: 0.7314\n",
      "Epoch 74/200\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5532 - acc: 0.7315\n",
      "Epoch 75/200\n",
      "25724/25724 [==============================] - 1s 56us/sample - loss: 0.5528 - acc: 0.7321\n",
      "Epoch 76/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5525 - acc: 0.7320\n",
      "Epoch 77/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5529 - acc: 0.7322\n",
      "Epoch 78/200\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5528 - acc: 0.7325\n",
      "Epoch 79/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5527 - acc: 0.7319\n",
      "Epoch 80/200\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5526 - acc: 0.7319\n",
      "Epoch 81/200\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5525 - acc: 0.7324\n",
      "Epoch 82/200\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5529 - acc: 0.7322\n",
      "Epoch 83/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5523 - acc: 0.7332\n",
      "Epoch 84/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5525 - acc: 0.7331\n",
      "Epoch 85/200\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5524 - acc: 0.7329\n",
      "Epoch 86/200\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5527 - acc: 0.7324\n",
      "Epoch 87/200\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.5525 - acc: 0.7317\n",
      "Epoch 88/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5524 - acc: 0.7328\n",
      "Epoch 89/200\n",
      "25724/25724 [==============================] - ETA: 0s - loss: 0.5516 - acc: 0.733 - 1s 58us/sample - loss: 0.5520 - acc: 0.7330\n",
      "Epoch 90/200\n",
      "25724/25724 [==============================] - 2s 60us/sample - loss: 0.5523 - acc: 0.73381s\n",
      "Epoch 91/200\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5521 - acc: 0.7326\n",
      "Epoch 92/200\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5521 - acc: 0.73090s - loss: 0.5548 -\n",
      "Epoch 93/200\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5516 - acc: 0.7323\n",
      "Epoch 94/200\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5523 - acc: 0.7323\n",
      "Epoch 95/200\n",
      "25724/25724 [==============================] - 2s 81us/sample - loss: 0.5518 - acc: 0.7327\n",
      "Epoch 96/200\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.5523 - acc: 0.7329\n",
      "Epoch 97/200\n",
      "25724/25724 [==============================] - 2s 83us/sample - loss: 0.5521 - acc: 0.7319\n",
      "Epoch 98/200\n",
      "25724/25724 [==============================] - 3s 99us/sample - loss: 0.5520 - acc: 0.7330\n",
      "Epoch 99/200\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5519 - acc: 0.7326\n",
      "Epoch 100/200\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5520 - acc: 0.73320s - loss: \n",
      "Epoch 101/200\n",
      "25724/25724 [==============================] - 3s 101us/sample - loss: 0.5516 - acc: 0.7332\n",
      "Epoch 102/200\n",
      "25724/25724 [==============================] - 3s 102us/sample - loss: 0.5517 - acc: 0.7334\n",
      "Epoch 103/200\n",
      "25724/25724 [==============================] - 3s 101us/sample - loss: 0.5517 - acc: 0.7331\n",
      "Epoch 104/200\n",
      "25724/25724 [==============================] - 3s 99us/sample - loss: 0.5514 - acc: 0.7334\n",
      "Epoch 105/200\n",
      "25724/25724 [==============================] - 3s 125us/sample - loss: 0.5517 - acc: 0.7336\n",
      "Epoch 106/200\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5518 - acc: 0.7331\n",
      "Epoch 107/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5518 - acc: 0.73280s - loss: 0.5519 - acc: 0.73\n",
      "Epoch 108/200\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5516 - acc: 0.73260s - loss: 0.552\n",
      "Epoch 109/200\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5515 - acc: 0.7335\n",
      "Epoch 110/200\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5513 - acc: 0.7325\n",
      "Epoch 111/200\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5514 - acc: 0.7335\n",
      "Epoch 112/200\n",
      "25724/25724 [==============================] - 2s 90us/sample - loss: 0.5516 - acc: 0.7328\n",
      "Epoch 113/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5517 - acc: 0.7332\n",
      "Epoch 114/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5514 - acc: 0.7336\n",
      "Epoch 115/200\n",
      "25724/25724 [==============================] - 1s 43us/sample - loss: 0.5515 - acc: 0.7320\n",
      "Epoch 116/200\n",
      "25724/25724 [==============================] - 2s 64us/sample - loss: 0.5514 - acc: 0.7324\n",
      "Epoch 117/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5515 - acc: 0.7327\n",
      "Epoch 118/200\n",
      "25724/25724 [==============================] - 2s 84us/sample - loss: 0.5514 - acc: 0.7327\n",
      "Epoch 119/200\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5511 - acc: 0.7328\n",
      "Epoch 120/200\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5512 - acc: 0.7313\n",
      "Epoch 121/200\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5514 - acc: 0.7337\n",
      "Epoch 122/200\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5513 - acc: 0.73390s - loss: 0.\n",
      "Epoch 123/200\n",
      "25724/25724 [==============================] - 1s 37us/sample - loss: 0.5512 - acc: 0.7332\n",
      "Epoch 124/200\n",
      "25724/25724 [==============================] - 1s 48us/sample - loss: 0.5512 - acc: 0.7334\n",
      "Epoch 125/200\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5508 - acc: 0.7331\n",
      "Epoch 126/200\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5511 - acc: 0.7328\n",
      "Epoch 127/200\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5508 - acc: 0.7323\n",
      "Epoch 128/200\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5509 - acc: 0.7324\n",
      "Epoch 129/200\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5509 - acc: 0.7331\n",
      "Epoch 130/200\n",
      "25724/25724 [==============================] - 1s 38us/sample - loss: 0.5510 - acc: 0.7334\n",
      "Epoch 131/200\n",
      "25724/25724 [==============================] - 1s 38us/sample - loss: 0.5506 - acc: 0.7332\n",
      "Epoch 132/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5505 - acc: 0.7338\n",
      "Epoch 133/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5506 - acc: 0.7324\n",
      "Epoch 134/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5509 - acc: 0.7329\n",
      "Epoch 135/200\n",
      "25724/25724 [==============================] - 1s 37us/sample - loss: 0.5510 - acc: 0.7340\n",
      "Epoch 136/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5509 - acc: 0.7335\n",
      "Epoch 137/200\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5504 - acc: 0.7348\n",
      "Epoch 138/200\n",
      "25724/25724 [==============================] - 2s 68us/sample - loss: 0.5505 - acc: 0.7338\n",
      "Epoch 139/200\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5503 - acc: 0.7348\n",
      "Epoch 140/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5505 - acc: 0.7343\n",
      "Epoch 141/200\n",
      "25724/25724 [==============================] - 3s 107us/sample - loss: 0.5506 - acc: 0.7329\n",
      "Epoch 142/200\n",
      "25724/25724 [==============================] - 2s 86us/sample - loss: 0.5507 - acc: 0.7329\n",
      "Epoch 143/200\n",
      "25724/25724 [==============================] - 2s 93us/sample - loss: 0.5502 - acc: 0.7332\n",
      "Epoch 144/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5505 - acc: 0.7336\n",
      "Epoch 145/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5513 - acc: 0.7344\n",
      "Epoch 146/200\n",
      "25724/25724 [==============================] - 1s 44us/sample - loss: 0.5508 - acc: 0.7334\n",
      "Epoch 147/200\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5507 - acc: 0.7333\n",
      "Epoch 148/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5505 - acc: 0.7335\n",
      "Epoch 149/200\n",
      "25724/25724 [==============================] - 2s 79us/sample - loss: 0.5505 - acc: 0.7336\n",
      "Epoch 150/200\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5506 - acc: 0.7332\n",
      "Epoch 151/200\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5501 - acc: 0.7340\n",
      "Epoch 152/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5504 - acc: 0.7333\n",
      "Epoch 153/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5506 - acc: 0.7337\n",
      "Epoch 154/200\n",
      "25724/25724 [==============================] - 1s 38us/sample - loss: 0.5505 - acc: 0.7334\n",
      "Epoch 155/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5502 - acc: 0.7331\n",
      "Epoch 156/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5505 - acc: 0.7336\n",
      "Epoch 157/200\n",
      "25724/25724 [==============================] - 1s 40us/sample - loss: 0.5505 - acc: 0.7334\n",
      "Epoch 158/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5503 - acc: 0.7335\n",
      "Epoch 159/200\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5502 - acc: 0.7329\n",
      "Epoch 160/200\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5503 - acc: 0.7337\n",
      "Epoch 161/200\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5504 - acc: 0.7341\n",
      "Epoch 162/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5503 - acc: 0.7329\n",
      "Epoch 163/200\n",
      "25724/25724 [==============================] - 1s 39us/sample - loss: 0.5503 - acc: 0.7335\n",
      "Epoch 164/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5500 - acc: 0.7338\n",
      "Epoch 165/200\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5501 - acc: 0.7341\n",
      "Epoch 166/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5501 - acc: 0.7341\n",
      "Epoch 167/200\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5501 - acc: 0.7330\n",
      "Epoch 168/200\n",
      "25724/25724 [==============================] - 2s 66us/sample - loss: 0.5499 - acc: 0.7343\n",
      "Epoch 169/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5502 - acc: 0.7335\n",
      "Epoch 170/200\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5501 - acc: 0.7339\n",
      "Epoch 171/200\n",
      "25724/25724 [==============================] - 1s 42us/sample - loss: 0.5500 - acc: 0.7337\n",
      "Epoch 172/200\n",
      "25724/25724 [==============================] - 3s 109us/sample - loss: 0.5502 - acc: 0.7336\n",
      "Epoch 173/200\n",
      "25724/25724 [==============================] - 1s 58us/sample - loss: 0.5507 - acc: 0.7333\n",
      "Epoch 174/200\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5505 - acc: 0.7328\n",
      "Epoch 175/200\n",
      "25724/25724 [==============================] - 2s 61us/sample - loss: 0.5505 - acc: 0.7333\n",
      "Epoch 176/200\n",
      "25724/25724 [==============================] - 2s 63us/sample - loss: 0.5503 - acc: 0.7340\n",
      "Epoch 177/200\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5500 - acc: 0.7330\n",
      "Epoch 178/200\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5501 - acc: 0.7321\n",
      "Epoch 179/200\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5499 - acc: 0.7322\n",
      "Epoch 180/200\n",
      "25724/25724 [==============================] - 2s 70us/sample - loss: 0.5504 - acc: 0.7335\n",
      "Epoch 181/200\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5504 - acc: 0.7327\n",
      "Epoch 182/200\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5501 - acc: 0.7332\n",
      "Epoch 183/200\n",
      "25724/25724 [==============================] - 2s 74us/sample - loss: 0.5498 - acc: 0.7341\n",
      "Epoch 184/200\n",
      "25724/25724 [==============================] - 1s 57us/sample - loss: 0.5501 - acc: 0.7330\n",
      "Epoch 185/200\n",
      "25724/25724 [==============================] - 1s 50us/sample - loss: 0.5497 - acc: 0.7340\n",
      "Epoch 186/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5502 - acc: 0.73340s - loss: 0.5527\n",
      "Epoch 187/200\n",
      "25724/25724 [==============================] - 1s 51us/sample - loss: 0.5504 - acc: 0.7331\n",
      "Epoch 188/200\n",
      "25724/25724 [==============================] - 1s 41us/sample - loss: 0.5499 - acc: 0.7335\n",
      "Epoch 189/200\n",
      "25724/25724 [==============================] - 1s 46us/sample - loss: 0.5502 - acc: 0.7341\n",
      "Epoch 190/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5498 - acc: 0.7344\n",
      "Epoch 191/200\n",
      "25724/25724 [==============================] - 3s 103us/sample - loss: 0.5503 - acc: 0.7332\n",
      "Epoch 192/200\n",
      "25724/25724 [==============================] - 1s 47us/sample - loss: 0.5501 - acc: 0.7336\n",
      "Epoch 193/200\n",
      "25724/25724 [==============================] - 1s 52us/sample - loss: 0.5501 - acc: 0.73360s - loss: 0.5508 - acc: 0.73\n",
      "Epoch 194/200\n",
      "25724/25724 [==============================] - 1s 53us/sample - loss: 0.5497 - acc: 0.7344\n",
      "Epoch 195/200\n",
      "25724/25724 [==============================] - 1s 55us/sample - loss: 0.5499 - acc: 0.7332\n",
      "Epoch 196/200\n",
      "25724/25724 [==============================] - 2s 59us/sample - loss: 0.5499 - acc: 0.73340s - loss: 0.5487 \n",
      "Epoch 197/200\n",
      "25724/25724 [==============================] - 2s 71us/sample - loss: 0.5496 - acc: 0.7344\n",
      "Epoch 198/200\n",
      "25724/25724 [==============================] - 1s 54us/sample - loss: 0.5502 - acc: 0.7332\n",
      "Epoch 199/200\n",
      "25724/25724 [==============================] - 1s 45us/sample - loss: 0.5500 - acc: 0.7341\n",
      "Epoch 200/200\n",
      "25724/25724 [==============================] - 1s 49us/sample - loss: 0.5499 - acc: 0.7338\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8575/8575 - 0s - loss: 0.5745 - acc: 0.7303\n",
      "Loss: 0.5745299908440593, Accuracy: 0.7302623987197876\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "print(h5py.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "filepath= r\"C:\\Users\\19802\\Desktop\\UNCC_bootcamp_03-23\\Challenge _module01\\module_21_deep_learning_21\\AlphabetSoupCharity.h5\"\n",
    "nn.save(filepath,save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.616416</td>\n",
       "      <td>0.694993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.571213</td>\n",
       "      <td>0.726053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.565102</td>\n",
       "      <td>0.726442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562683</td>\n",
       "      <td>0.728269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.561945</td>\n",
       "      <td>0.728114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.552325</td>\n",
       "      <td>0.732507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.552352</td>\n",
       "      <td>0.733945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.552612</td>\n",
       "      <td>0.732545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.552251</td>\n",
       "      <td>0.732157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.552338</td>\n",
       "      <td>0.732118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       acc\n",
       "0   0.616416  0.694993\n",
       "1   0.571213  0.726053\n",
       "2   0.565102  0.726442\n",
       "3   0.562683  0.728269\n",
       "4   0.561945  0.728114\n",
       "..       ...       ...\n",
       "95  0.552325  0.732507\n",
       "96  0.552352  0.733945\n",
       "97  0.552612  0.732545\n",
       "98  0.552251  0.732157\n",
       "99  0.552338  0.732118\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorse =pd.DataFrame(fit_model.history)\n",
    "scorse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2049f4b2908>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  plot the loss\n",
    "scorse.plot(y='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2049dec9308>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  plot the accuracy\n",
    "scorse.plot(y='acc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
